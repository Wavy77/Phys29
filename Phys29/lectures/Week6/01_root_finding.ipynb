{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Roots of Nonlinear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we will study how to  to numerically solve the equation $f(x)=0$, where $f(x)$ is some function.  A solution $x$ that satisfies this equation is\n",
    "known as a *root* of the equation.  Depending on the function $f(x)$, there could be no roots, one root, or multiple roots, and this immediately \n",
    "suggests that before you even start coding up this problem, you should investigate the behavior of the function so that you know what you are asking\n",
    "the computer to do.  The best way of doing this is to simply plot the function using `matplotlib.pyplot`, so that you have an idea of how\n",
    "many roots you might have, and approximately where they might be.  Having this information before attempting a solution is incredibly valuable.\n",
    "\n",
    "Incidentally,  a closely related problem is to calculate the locations of minima, maxima, and points of inflection of a function, i.e the roots of\n",
    "$f^\\prime(x)=0$.  This can also be done using the methods we will explore here.\n",
    "\n",
    "There are a variety of different numerical methods that you can use to find roots, and choosing between them depends on how much knowledge you have about the root, the functional behavior near the root, and whether or not you can compute the derivative of the function.  Some algorithms are more efficient than others, for *most* usage cases, but they can also fail sometimes.  Again, the more knowledge you have of the function's behavior, the better off you will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kepler's Equation\n",
    "As a first example, consider a famous historical example of a transcendental equation in physics and astronomy:\n",
    "$$\n",
    "M=E-e\\sin E\n",
    "$$\n",
    "which is known as *Kepler's equation*, and concerns planets on eccentric orbits around the sun, where the eccentricity $e$ lies between 0 (circular\n",
    "orbit) and 1 (parabolic orbit).  The historical problem<sup id=\"a1\">[1](#f1)</sup> was to calculate $E$ (the so-called eccentric anomaly) given a value of $M$ (the so-called mean anomaly).  There is no exact closed-form analytic solution, although there are solutions that can be expressed in terms of infinite series of special functions - not very practical!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try and solve this equation for $E$ when $M=1$ radian and eccentricity $e=0.4$.   Before we try to solve for the roots, we should plot the function to better undersand what we are dealing with. Rearranging\n",
    "to the form $f(x)=0$ we get:\n",
    "$$ \n",
    "E-e\\sin E-M = 0\n",
    "$$\n",
    "and define this to be the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def kepler(E, M=1.0, e=0.4):\n",
    "    \"\"\"\n",
    "    Kepler's equation function for root finding. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    E : float or array_like\n",
    "        Eccentric anomaly in radians. \n",
    "    M : float or array_like, optional\n",
    "        Mean anomaly in radians. The default is 1.0.\n",
    "    e : float or array_like, optional\n",
    "        Eccentricity (dimensionless). The default is 0.4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    keplers_eq : float or array_like\n",
    "        Result of Kepler's equation in radians. \n",
    "    \"\"\"\n",
    "\n",
    "    return E - e*np.sin(E) - M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "E = np.linspace(0, np.pi, 100)\n",
    "kepler_eq = kepler(E)\n",
    "plt.plot(E, kepler_eq)\n",
    "plt.xlabel('E (radians)')\n",
    "plt.ylabel('Kepler(E)')\n",
    "plt.title('M=1 radians, e=0.4')\n",
    "plt.axhline(0, color='black', linestyle=':', lw=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently there is a single root at around $E\\approx 1.4$ radians. Let us know explore two different methods to find it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bracketing and Bisection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases where $f(x)=0$ and $f^\\prime(x)\\ne0$, the function will have opposite signs on either side of the root.  In this case we can **bracket** the root\n",
    "by saying it must lie somewhere between a value $x=a$ where $f(a)$ has one sign, and a value $x=b$ where $f(b)$ has the opposite sign.  For such cases\n",
    "there is a very simple method of finding the root known as **bisection**. It is generally rather inefficent, but unlike some other methods (including\n",
    "the next one), it is **guaranteed** to work.\n",
    "\n",
    "The idea is very simple.  Just divide the interval $a<x<b$ evenly in two, and check the sign of the function at the midpoint:  $f((a+b)/2)$.  The sign\n",
    "of the function there then tells you in which half the root lies, so you have produced a tighter bracket.  Then just iterate the same procedure \n",
    "until your bracket achieves your desired level of accuracy. \n",
    "\n",
    "The code below provides an example implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection(func, a, b, tol=1e-4, max_iter=100, verbose=False):\n",
    "    \"\"\"\n",
    "    Find a root x of the equation func(x)=0 within an interval [a, b] using the bisection method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : callable\n",
    "        The function for which to find the root. \n",
    "    a : float\n",
    "        The lower bound of the interval.\n",
    "    b : float\n",
    "        The upper bound of the interval.\n",
    "    tol : float, optional\n",
    "        The absolute tolerance for the root. The function returns when the size of the bisection interval is less than tol. Default is 1e-4. \n",
    "    max_iter : int, optional\n",
    "        The maximum number of iterations. Default is 100.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    root: float\n",
    "        The root of the function.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If a is not less than b.\n",
    "    ValueError\n",
    "        If f(a) and f(b) have the same sign.\n",
    "    ValueError\n",
    "        If the function does not converge within max_iter iterations.\n",
    "    \"\"\"\n",
    "    # Do some error checking\n",
    "    if not a < b:\n",
    "        raise ValueError(\"a must be less than b\")\n",
    "    if func(a) * func(b) > 0:\n",
    "        raise ValueError(\"f(a) and f(b) must have different signs\")\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        # bisect, x is the new candidate root\n",
    "        x = (a + b) / 2\n",
    "        if verbose:\n",
    "            # Print the current values of a, root, and b\n",
    "            print(f\"iteration: {iter + 1:3d}, a: {a:.6f}, root: {x:.6f}, b: {b:.6f}, eps: {(b - a)/2:.6f}\")\n",
    "        # Check if we found the zero or if the localization interval (b-a)/2 = (x-a) = (b-x) \n",
    "        # is within the tolerance.\n",
    "        if func(x) == 0.0 or (b - a)/2 < tol:\n",
    "            break\n",
    "        # If the signs of f(x) and f(a) are different then the root is between a and x, update b\n",
    "        if func(x) * func(a) < 0:\n",
    "            b = x\n",
    "        # Otherwise if the sign of f(a) and f(x) are the same, the root is between x and b, update a\n",
    "        else:\n",
    "            a = x\n",
    "    else:\n",
    "        # If the loop completes without a break, enter the else and raise an error.\n",
    "        raise ValueError(f\"The localization interval = {b-a/2} > {tol} = tolerance after max_iter = {max_iter} iterations\\n\" + \n",
    "                         f\"Increase the value of max_iter. \")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the bracket interval decreases by a factor of two on every iteration, then after $n$ iterations, the original interval $\\epsilon_0=b-a$ will be reduced to \n",
    "$$\n",
    "\\epsilon_n=2^{-n}\\epsilon_0,  \n",
    "$$\n",
    "where $\\epsilon_n$ is the width of the bracket at the $n$ th iteration.  In other words, achieving a tolerance of $\\epsilon_n$ requires \n",
    "$$\n",
    " n = \\ln(\\epsilon_0/\\epsilon_n)/\\ln2, \n",
    "$$\n",
    "iterations, and the better your initial guess for the root, $\\epsilon_0$, the fewer iterations required. Because $\\epsilon_{n+1}$ is simply proportional to $\\epsilon_n$\n",
    "$$\n",
    "\\epsilon_{n+1} = \\epsilon_n/2, \n",
    "$$\n",
    "the bisection method is said to converge linearly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run on Kepler's equation in verbose mode and print the results to the screen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By eye the root was around 1.4 so we choose the values of a and b accordingly\n",
    "a = 1.25\n",
    "b = 1.5\n",
    "\n",
    "root = bisection(kepler, a, b, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it took 12 iterations to achieve our tolerance of `tol=1e-4` or $\\epsilon_n=10^{-4}$. Since $b=1.5$ and $a = 1.25$, \n",
    "$\\epsilon_0 = b-a = 1.5-1.25 = 0.25$, and we have\n",
    "$$\n",
    "n = \\ln({0.25/10^{−4}})/\\ln 2 = 11.3, \n",
    "$$\n",
    "so 12 iterations are required. With that number we achieve a tolerance of $0.25/2^{12} = 6.1 \\times 10^{−5}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Newton-Raphson Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is a method that does not require that you bracket the root.  Instead, the idea is extremely simple: starting from an initial guessed value of the root $x_0$, Taylor expand your function to first (linear) order about this guess and set that Taylor expansion to zero to find a new guess for the root:\n",
    "$$\n",
    "f(x)=0\\simeq f(x_0)+f^\\prime(x_0)(x-x_0),\n",
    "$$\n",
    "which gives\n",
    "$$\n",
    "x=x_0-\\frac{f(x_0)}{f^\\prime(x_0)}.\n",
    "$$\n",
    "In other words, you approximate your function with a linear approximation, also known as the tangent line at your guessed value, and find where the tangent line intercepts the $x$-axis to\n",
    "find a new guess for your root.  Then we just iterate:\n",
    "$$\n",
    "x_{n+1}=x_n-\\frac{f(x_n)}{f^\\prime(x_n)}.\n",
    "$$\n",
    "\n",
    "The disadvantage of this method is that if you ever happen to get near an extremum of your function, where $f^\\prime(x_n)$ is close to zero, this\n",
    "method will then take you very far away from where the root is likely to be. There are also situations where Newton-Raphson iterations can oscillate\n",
    "about the root but not converge to it. Thus Newton-Raphson can fail or take a very long time to converge in some instances. \n",
    "\n",
    "On the other hand, when it works, Newton-Raphson converges much more rapidly than bisection, provided the\n",
    "function is smooth and you are sufficiently close to the root. You can see this just by Taylor expanding the numerator and denominator of the\n",
    "expression below:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\epsilon_{n+1} & \\equiv x_{n+1}-x_n = -\\frac{f(x_n)}{f^\\prime(x_n)} = -\\frac{f(x_{n-1}+\\epsilon_n)}{f^\\prime(x_{n-1}+\\epsilon_n)} \\\\\n",
    "& = -\\frac{f(x_{n-1})+\\epsilon_nf^\\prime(x_{n-1})+\\frac{\\epsilon_n^2}{2} f^{\\prime\\prime}(x_{n-1})+...}{f^\\prime(x_{n-1})+\\epsilon_n f^{\\prime\\prime}(x_{n-1}) + \\frac{\\epsilon_n^2}{2}f^{\\prime\\prime\\prime}(x_{n-1})+ ...} = -\\frac{\\frac{\\epsilon_n^2}{2} f^{\\prime\\prime}(x_{n-1})+...}{f^\\prime(x_{n-1})+\\epsilon_n f^{\\prime\\prime}(x_{n-1}) + \\frac{\\epsilon_n^2}{2}f^{\\prime\\prime\\prime}(x_{n-1}) + ...}\\\\\n",
    "& \\simeq -\\frac{\\epsilon_n^2}{2}\\frac{f^{\\prime\\prime}(x_{n-1})}{f^\\prime(x_{n-1})}, \n",
    "\\end{align*}\n",
    "$$\n",
    "where we used the Newton-Raphson formula in the second line, $f(x_{n-1})+\\epsilon_nf^\\prime(x_{n-1})=0$, to simplify the numerator, and for the approximation on the third line we assumed we were very close to the root, which means $\\epsilon_n$ is a small number, and thus we kept only the leading order terms $\\epsilon_n$. \n",
    "\n",
    "In contrast to bisection, where $\\epsilon_{n+1}=\\epsilon_n/2$ (linear in $\\epsilon_n$), here $\\epsilon_{n+1}$ is *quadratic* in $\\epsilon_n$.\n",
    "Thus provided we are close enough to the real root so that the equation above is valid (recall we assumed $\\epsilon_n$ is small, which means that\n",
    "successive guesses $x_n$ and $x_{n-1}$ are close to each other), Newton-Raphson will converge much faster than bisection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide an example of a Newton-Raphson implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv(func, x, h=1e-4):\n",
    "    \"\"\"\n",
    "    Compute the nth derivative of a function f(x) using the symmetric difference formula.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : callable \n",
    "        Function to take the derivative of.\n",
    "    x : float or array_like\n",
    "        Location(s) at which to evaluate the derivative.\n",
    "    h : float, optional\n",
    "        Step size. The default is 1e-4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df/dx(x): float or array_like\n",
    "        The derivative of f(x) evaluated at x. shape is same as x. \n",
    "\n",
    "    \"\"\"\n",
    "    return (func(x + h) - func(x - h))/(2*h)\n",
    "\n",
    "\n",
    "def newton_raphson(func, x0, fprime=None, tol=1e-4, h=1e-4, max_iter=100, verbose=False):\n",
    "    \"\"\"\n",
    "    Find a root x of the equation func(x)=0 with derivative df(x) using the Newton-Raphson method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : callable\n",
    "        The function for which to find the root.\n",
    "    x0 : float\n",
    "        The initial guess for the root.\n",
    "    fprime : callable, optional \n",
    "        The derivative of the function. Default is None, in which case the derivative will be computed numerically.  \n",
    "    tol : float, optional\n",
    "        The absolute tolerance for the root. The function returns when the absolute value of the difference \n",
    "        between the current and previous root is less than tol. Default is 1e-5.\n",
    "    h : float, optional\n",
    "        Step size if the derivative is to be computed numerically. The default is 1e-4.\n",
    "    max_iter : int, optional\n",
    "        The maximum number of iterations. Default is 100.\n",
    "    verbose : bool, optional\n",
    "        If True, print the iteration process. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The root of the function.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the function does not converge within max_iter iterations.\n",
    "    \"\"\"\n",
    "    # Initialize the current guess for the root\n",
    "    x = x0\n",
    "\n",
    "    # Perform the Newton-Raphson iteration\n",
    "    for iter in range(max_iter):\n",
    "        # Calculate the function value and its derivative at the current guess\n",
    "        f = func(x)\n",
    "        # Use the analytical derivative function if provided, otherweise use the numerical derivative\n",
    "        dfx = fprime(x) if fprime is not None else deriv(func, x, h=h)\n",
    "\n",
    "        # Update the guess using the Newton-Raphson formula\n",
    "        prev_x = x\n",
    "        x = x - f/dfx\n",
    "\n",
    "        if verbose:\n",
    "            # Print the current iteration, root, and absolute difference between the current and previous root\n",
    "            print(f\"iteration: {iter + 1}, current root: {x:.12f}, previous root: {prev_x:.12f}, |difference|: {np.abs(x - prev_x):.12f}\")\n",
    "\n",
    "        # If the absolute difference between the current and previous root is less than the tolerance, the root has been found\n",
    "        if func(x) == 0.0 or np.abs(x - prev_x) < tol:\n",
    "            break \n",
    "    else: \n",
    "        # If the function did not break out after max_iter iterations we enter elese. Since it did not converge. Raise an exception. \n",
    "        raise ValueError(f\"The absolute difference between the current and previous root = {abs(x - prev_x)} > {tol} = tolerance after max_iter = {max_iter} iterations\\n\" + \n",
    "                         f\"Increase the value of max_iter or changing the initial guess.\")\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the code above, a function `df` that computes the derivative of the `f(x)` is optional. If the function `df` is not provided, the code defaults to computing $f^\\prime(x)$ numerically using the function `deriv`, which uses the symmetric difference derivative estimate that you studied in the homework.  Generally speaking, it is always better to provide the derivative function if it can be computed analytically, since this avoids extra floating point errors associated with taking numerical derivatives.  However, in a situation where an analytic closed form expression for the derivative of your function does not exist, a numerical derivative would be the way to go. Indeed, application of the Newton-Raphson algorithm with a numerically estimated derivative is closely related to the  **secant method**, which is a root-finding algorithm that is qualitatively similar to Newton-Raphson, but does not require the derivative of the function, since it estimates the derivative using a finite difference of the function over an interval which becomes smaller and smaller as the algorithm iterates.  Passing in the derivative function as an optional argument is analogous to the way the more sophisticated root-finding routines in `scipy.optimize` work, which we will come to next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide examples of our Newton-Raphson implementation for both cases, i.e. with and without passing in an explicit derivative function. First, the function implementing the analytical derivative of the Kepler equation is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kepler_prime(E, e=0.4):\n",
    "    \"\"\"\n",
    "    Returns the derivative of Kepler's equation for the eccentric anomaly E. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    E : float or array_like\n",
    "        Eccentric anomaly in radians. \n",
    "    e : float or array_like, optional\n",
    "        Eccentricity (dimensionless). The default is 0.4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    keplers_eq_deriv : float or array_like\n",
    "        Derivative of Kepler's equation with respect to E (dimensionless). \n",
    "    \"\"\"\n",
    "\n",
    "    return 1 - e*np.cos(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the example passing in the derivative function explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E0 = 1.0\n",
    "root = newton_raphson(kepler, E0, fprime=kepler_prime, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now computing the derivative numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_secant = newton_raphson(kepler, E0, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we started further away from the answer $E_0 = 1.0$ than when we tried solving using bisection, Newton-Raphson converged to the same tolerance in just four iterations! This is the case whether or not we passed in the derivative function explicitly. But one sees that some of the numbers in the numerical derivative version differ slightly at the level of the 12th decimal place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Finding Using Scipy or Numpy\n",
    "\n",
    "The example codes provided in this notebook are here to introduce the basic ideas that underlie root-finding. However, in practice, you should use the root-finding routines provided in the `scipy.optimize` module, which are far more sophisticated and robust than the simple implementations we have provided here.  The `scipy.optimize` module provides a variety of different root-finding routines, including the bisection and Newton-Raphson methods we  discussed, as well as other more sophisticated methods that are extensions of these ideas. The `numpy` module also has a root-finding routine specifically designed for finding the roots of polynomials, which can be determined far more quickly and robustly using algorithms specifically designed for polynomial roots. \n",
    "\n",
    "In this lecture we only considered finding roots of one dimensional (univariate) functions $f(x)=0$, but sometimes we are interested in finding the multidimensional (multivariate) roots of a coupled system of equations, e.g. the values of $x$ and $y$ that simultaneously solve $f(x,y)=0$ and $g(x,y)=0$.  Or more generally in $N$ dimensions\n",
    "$$\n",
    "f_i(\\mathbf{x})\\equiv f_i(x_0,x_1,...,x_{N-1})=0,\n",
    "$$\n",
    "where the index $i$ can run from 0 to $N-1$, and both ${\\bf f}$ and ${\\bf x}$ are $N$-dimensional vectors.  This problem is famously difficult, in that there are *no* known algorithms that are guaranteed to work to find all the roots. Multidimensional Newton-Raphson is generally the workhorse here, but it can fail for similar reasons that it can fail in computing one-dimensional roots.   We will not discuss multivariate root-finding further, but it is worth noting that the generalization  of the derivative in one dimensional Newton-Raphson to the multidimensional case is the Jacobian matrix\n",
    "$$\n",
    "J_{ij}=\\frac{\\partial f_i}{\\partial x_j},  \n",
    "$$\n",
    "which can either be provided as an input function (analogous to our `fprime` function in the one-dimensional case), or it can be estimated \n",
    "numerically in the `scipy.optimize` routines. \n",
    "\n",
    "For root-finding using `scipy` or `numpy` the same basic principles apply: \n",
    "1.  plot the function to get an idea of where the roots are, and what algorithm will be most appropriate \n",
    "2.  routines that require a bracket will always perform better if the interval is close to the root \n",
    "3.  if the algorithm requires derivative information, analytic derivatives are typically going to perform better than numerical derivatives. \n",
    "\n",
    "Below we provide a summary of the different root-finding routines available in `scipy.optimize` and `numpy`, along with some of the pros and cons of each method:\n",
    "\n",
    "1. **`scipy.optimize.root`**\n",
    "    - **Pros**: Can handle both univariate and multivariate functions. Supports several methods (Hybrid, LM, Broyden, etc.), and automatically chooses the best method if none is specified. Provides detailed output information.\n",
    "    - **Cons**: Might be overkill for simple univariate functions. Some methods require the Jacobian matrix. \n",
    "\n",
    "2. **`scipy.optimize.newton`**\n",
    "    - **Pros**: Fast convergence for functions that are well-behaved. Will fall back to the secant method if the derivative is not provided.\n",
    "    - **Cons**: Requires the derivative of the function. Can fail to converge or converge to a non-root if the initial guess is not close to the root.\n",
    "\n",
    "3. **`scipy.optimize.bisect`**\n",
    "    - **Pros**: Guaranteed to converge if the function changes sign over an interval. Does not require the derivative of the function.\n",
    "    - **Cons**: Slower convergence compared to methods like Newton or secant.\n",
    "\n",
    "4. **`scipy.optimize.ridder`**\n",
    "    - **Pros**: Faster convergence than bisection. Does not require the derivative of the function.\n",
    "    - **Cons**: Requires the function to change sign over the specified interval.\n",
    "\n",
    "5.  **`scipy.optimize.brentq` and `scipy.optimize.brenth`**\n",
    "    - **Pros**: Combines bisection, secant, and so-called inverse quadratic interpolation. Generally considered a good choice for most numerical root-finding problems. Does not require the derivative of the function.\n",
    "    - **Cons**: Requires the function to change sign over an interval.\n",
    "\n",
    "6.  **`scipy.optimize.fsolve`**\n",
    "    - **Pros**: Can handle both univariate and multivariate functions. Similar to `root` but provides a simpler interface for common use cases.\n",
    "    - **Cons**: Might be less robust than `root`. Some methods require the Jacobian matrix. \n",
    "\n",
    "7. **`scipy.optimize.root_scalar`**\n",
    "    - **Pros**: Provides a unified interface for several different root-finding methods, including bisection, Brent's method, Ridder's method, and secant method. Can handle univariate functions. Does not require the derivative of the function for some methods.\n",
    "    - **Cons**: Requires the function to change sign over an interval for methods like bisection, Brent's method, and Ridder's method. For the secant method, requires a good initial guess to ensure convergence.\n",
    "8. **`numpy.roots`**\n",
    "    - **Pros**: Specifically designed for finding the roots of polynomials. Can find all roots of a polynomial, including complex roots.\n",
    "    - **Cons**: Only works for polynomials. The accuracy of the roots can be affected by the numerical stability of the polynomial's coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example using the scipy.optimize.bisect function to solve Kepler's equation. I've intentionally set the absolute tolerance to be `1e-4` and the initial guess interval $a$ and $b$ to be identical to our example above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize \n",
    "# Find the root using the bisect function\n",
    "root_bisect, result_bisect = optimize.bisect(kepler, a, b, xtol=1e-4, full_output=True)\n",
    "\n",
    "# Print the root\n",
    "print(f\"The root is {root_bisect:.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can  check whether the root finding was successful and determine the number of function calls required via the returned `result` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bisect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Newton-Raphson we can use the `scipy.optimize.newton`, again with parameters analogous to what we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Kepler's equation using scipy.optimize.newton\n",
    "root_newton, result_newton = optimize.newton(kepler, E0, tol=1e-4, fprime=kepler_prime, full_output=True)\n",
    "\n",
    "# Print the root\n",
    "print(f\"The root is {root:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we could use `scipy.optimize.root_scalar` function (general univariate root finder) using **Brent's method** which combines bisection, the secant method, and inverse quadratic interpolation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_root = optimize.root_scalar(kepler, bracket = (a,b), method='brentq', xtol=1e-4)\n",
    "result_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Brent's method is a bracketed method which performs better than pure bisection, i.e. it finds the root in fewer iterations (4 vs 12). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"f1\" style=\"font-size: 1em;\"><small>[$^1$](#a1) The quantity $M$ is the *mean anomaly*, an angle given by the average angular velocity of the planet ($=\\sqrt{GM_\\odot/a^3}$, where $a$ is the semimajor axis of the ellipse and $M_\\odot$ is the mass of the sun) times the time since perihelion passage.  $E$ is a different angle known as the *eccentric anomaly*, and tells you where the planet is in its orbit: the cylindrical coordinates $r$ and $\\theta$ are given by\n",
    "$$\n",
    "r=a(1-e\\cos E)\\,\\,\\,\\,\\,{\\rm and}\\,\\,\\,\\,\\,\\cos\\theta=\\frac{\\cos E-e}{1-e\\cos E}\n",
    "$$\n",
    "Hence if you want to know where the planet is in its orbit at some given time\n",
    "(related to $M$), you have to solve Kepler's equation for $E$ and then\n",
    "substitute into these equations.  [Note that $M=E=\\theta$ at perihelion (0)\n",
    "and aphelion ($\\pi$).] Many famous mathematicians in history devoted effort to solving\n",
    "Kepler's equation! </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phys29",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
